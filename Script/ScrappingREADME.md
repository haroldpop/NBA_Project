To scrap the data, you will have to install beautiful soup, and selenium. Also, to use selenium, you will have to download a webdriver, I suggest you the chrome one,
available at this link: https://chromedriver.chromium.org/downloads (the windows version looks just for 32 bits Windows but work also for 64 bits). Once this webdriver
downloaded, you will have to unzip it and place it in the same folder than the script. (It was the easiest way for me to do it with jupyter notebook on my local).

Once you downloaded every package, you can start running the code from RUN HERE till SCRAP ALL DATAS part. 

I think we can split the scrapping like this: I will scrap 2010, 2011, 2012 and 2013 seasons, Anuj will scrap 2014, 2015, and 2016 and Anupam will scrap 2017, 2018 and 2020 ones.


BEFORE RUNNING BE SURE YOU SCRAP THE GOOD SEASONS. 

2014, 2015 and 2016 seasons means that you will RUN THE CELL FOR S_watch[2:5] !!!!! not S_watch only

2017, 2018 and 2020 means that you will RUN FOR S_watch[5:] !!!! 

After that you can run cells  just under for the other season, which means for other seasons haha. 
